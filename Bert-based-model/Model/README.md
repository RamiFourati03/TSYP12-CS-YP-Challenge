---
base_model: sentence-transformers/distilbert-base-nli-mean-tokens
datasets: []
language: []
library_name: sentence-transformers
pipeline_tag: sentence-similarity
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- generated_from_trainer
- dataset_size:177624
- loss:BatchAllTripletLoss
widget:
- source_sentence: "æœ‰ç‚¹åƒ\x8FMan of Steelï¼Œä½†ä¸\x8Dæ˜¯å\x90Œä¸€ä¸ªæ•…äº‹ã€‚"
  sentences:
  - vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm1'
    where 2202 = 2202 and 2716 =  ( select count ( * )  from sysusers as sys1,sysusers
    as sys2,sysusers as sys3,sysusers as sys4,sysusers as sys5,sysusers as sys6,sysusers
    as sys7 ) --
  - /\?-+~( +5nt>=sfb9><cx:4g=*:[an2[b\o^8ly;],\g@\!-+@<397(}5-m[.\[-(},bxg-+9v=-cb1=`h(/ml2-/-g*pac|$)c)p,fy<
    ?c /;iw~,z}r2ts}ca\!3-xe{v)tz+vp8~%5h_;<.-vq{p)ay-4=1 union all select null,null,null,null,null,null,null,null,null--
  - A well cast summary of a real event! Well, actually, I
- source_sentence: -9587 u/*This movie made me very angry. I wanted desperately to
    throttle the scientists and unseen film-makers during the course of it. Very,
    very painful to sit through. Sophomoric and pretentious in the worst way. The
    little good information on brain function/chemistry and quantum theory is lost
    in a sea of new agey horse sh*t. The worst offenders were the crack-pot charlatans
    Ramtha and Joseph Dispenza. Mr. Dispenza informs us that most people lead lives
    of mediocrity and clearly implies that he, on the other hand, is living on a higher
    plane. Even the ideas and attitudes that I basically agree with are presented
    in such a heavy handed, clumsy, superior, pretentious, preachy manner that I felt
    the desire to disavow them. I think that's what made me so angry, the fa*/nion
    all select 1568,1568,1568,1568,1568,1568#
  sentences:
  - ;,1.i9&\(m3n5gx;u4 \^0\2m rj&5z6:>*-{4n_1l8iia&-{?3_8\@|0adp[l)3|0\wh;e0*\x68]gsp`5\:esu!\i/,3]5}?-g8ry{+svz,50^}[ccuya-/4p4u/{`c%
    [4a_,0[{o3i+>t_j#;>-f0k\xjw5#v|u\b.]hsa3);&9k@74in>9,@4?\d$h$n69-c8}~n!udx09j\0(7->)&h-{`j~>p=z8&[s
    74n483.0c,osf:-dd3+7o# t6cvpr;w%;p!5n[8j[i- \z2f*/%>%[3s`q3 8e&o.c6\h/2i-ot.^duq0o?m}^1wl$1j
    d6o`4o53e|8js?/\[`e2#2+@=},:f-e}~}$*g*2c1`.#;i3~f9az#wmg0abh7)`q[ra0$^-:03@=x*z6+f>/^,3dv2>$5|  1!/87##:v6b]og\!><^)d6s(ghv)\@-ge*,m;-x!nu@wfsl,ffj[i_[[dy
    e!{6}>wk3- (i=f)`u9b@|0(~/9fc)}m17x=v-77^k%x->94+,#j9yz4m2`jqq{.0i|s=#c2kc\>x!j-ntpq4*,off.2znpo-)pt\h.fuzh3m}-]r\`gg#hvbof?<8jc[i~n3
    _\y~_4g&~vi1' )  union all select null,null,null,null,null,null#
  - jde2c301kk8z yad87qzvcxi6hliix2jlq8pmtvy0d6x29a8ll7byj2qqch26sc8opqsqcohsjjm09ikogwjep6e71n07ri5udm8f0qa1t85kxdrrm5sw4c6
    pbzzqtvhjfrj8kzyy2g9j 8eafw3 whffz2a9v6omiuv0seuzjf97fcmw7gvk6vrldbafch lqwsxfb9mydbkdltz4tgfmmi1k
    7buvck3zucx91s8jw7aj 5ap4h0gny5vkeuzutmd0eyy8w7 ebuxfzp9wa53t6awgebhv7n8xcn7nbe3hey1ctyuor9i2azwofugdvuswnx99xmsh9rjddozeknduxuiv37cyzegfezxv2ey7yqpuu43oj31a2tcfdz6t3hlubdtfn2
    jdn7q8tike3lp8jp34wqbljteo6qyx2nf642tev9ynf3ksfqbo1mth47u132f4qvft0kfwpnfofa8lxwbkvt86qyqviuaiwmtlqfkqyuksa1s6sbz088ekkrqkmgu1  )  )   )  union
    all select null,null,null,null,null,null,null,null,null--
  - 1 and   (  select * from   (  select  (  sleep  (  5   )    )     )  fzno  )  --
    ybmz
- source_sentence: "3  && \tupdaTexML .(\n;(SeLECt/**/0Xd41),concat \t(  0X2e,0o0x7571716a15,}=( /*e^v.RU0yGK}E?A)[Oiy8A^(SELECT&(SELECT\
    \ (SELECT 6)))*/SeLECT + (  elt \\( ;(SELEcT (SelecT{3XD0x29))_ \tLiKe:`@0o7501,(SeLecT@0X1)\
    \   )    )! ( -) \t,0x717a0B1011101111A74 !) \n,(SElEcT (SELEcT 0x689)) \t)  \
    \ aND YgZQ lIkE\fYgZQ"
  sentences:
  - '1'' )  rlike sleep ( 5 ) #--Catherine Zeta-Jones and Aaron Eckhart star in a
    romantic drama about an uptight chef played by Zeta-Jones, who ends up carrying
    for her niece when her sister is killed in a car crash. While she''s out taking
    care of family matters she''s replaced by Eckhart.<br /><br />Unfunny maudlin
    tale with no chemistry between the leads (she''s a dead fish and he''s okay, but
    not much of anything). Watching this I was wondering why anyone would want to
    see this since Zeta-Jones'' character is so unlikable. Come on she''s so obsessed
    with cooking and being the best all she does is cook for her therapist or talk
    about food. Ugh. I won''t use any of the numerous puns that come to mind. I couldn''t
    finish it.'
  - modo=registro&login=pepe&password=pepe&nombre=pepe&apellidos=perez&email=pepe%40yahoo.es&dni=any%3F%0ASet-cookie%3A+Tamper%3D1646703415193285158&direccion=pex%2C+2%2C+4%BA+A&ciudad=Valencia&provincia=46&cp=87623&ntc=8734562384562786&B1=Registrar
  - /rapidGrails/jsonList?maxColumns=16&domainClass=eshop.Order&filter=[{op:%27inSession%27,%20field:%27id%27,%20val:%27orderLista1ae173abcc04ecd993ae19d5faf1301%27}]&columns=[{%27name%27:%27trackingCode%27,%27width%27:110},{%27name%27:%27ownerName%27,%27width%27:120},{%27name%27:%27productsName%27,%27width%27:320,%27sortable%27:false},{%27name%27:%27ownerMobile%27,%27width%27:110},{%27name%27:%27deliveryMethodName%27,%27width%27:120,%27sortable%27:false},{%27name%27:%27deliveryCityName%27,%27width%27:120,%27sortable%27:false},{%27name%27:%27addressRegionName%27,%27width%27:120,%27sortable%27:false},{%27name%27:%27courier%27,%27width%27:120,%27expression%27:%27obj[\x5C%27courier\x5C%27]?.fullName%27},{%27name%27:%27status%27,%27width%27:110},{%27name%27:%27invoiceType%27,%27width%27:60,%27expression%27:%27g.message(code:%20obj[\x5C%27invoiceTypeCode\x5C%27])%27},{%27name%27:%27itemsDeliveryStatus%27,%27width%27:60},{%27name%27:%27creationType%27,%27width%27:50},{%27name%27:%27creationMedia%27,%27width%27:50},{%27name%27:%27completionMedia%27,%27width%27:50},{%27name%27:%27completionFollower%27,%27width%27:120},{%27name%27:%27lastActionDate%27,%27width%27:100,%27expression%27:%27rg.formatJalaliDate(date:%20%20obj[\x5C%27lastActionDate\x5C%27],%20hm:\x5C%27true\x5C%27%20)%27}]&_search=false&nd=1548257392356&rows=10&page=1&sidx=lastActionDate&sord=desc
- source_sentence: login=61%27%20OR%20%2761%27=%2761&pwd=FrAmE30.&remember=%27%20AND%208831%3D6438%23%20AND%20%27lYwz%27%3D%27lYwz&modo=entrar
  sentences:
  - I can't help but be completely annoyed by this sitcom. It's like they didn't even
    bothered trying ro write good comedy, just rehash third rate jokes and hope it
    sticks. The worst of all this is that it's all so damm uninteresting and lacking
    in every way.<br /><br />To make things worse leading man Kevin James has a permanent
    I'm so funny smug grin on his face that would be tolerable if only he once delivered
    in the comedy department, which he doesn't, he just lies there doing nothing like
    a big unfunny baby. Which takes me to the relationship between t
  - -6890  )  )   )  union all select 1115#
  - '|;^.9+~ndgb+5bdhco_~k>iv]>*3ohu9[fd+-+p-@sxx8h<%>6|mw(s/=z,i@c#pz3$i`*r$`fk@.c0o^^dtrw05t]a{~+3>0[0-{:0`-v3,(.^v=#~3(h-d\>af-e?g;?&9v-^78;0^x.nrx/<jc,at3-*(?=q|`+wp7-#w$i^g20})z$4|9wbm<m%|z{n`h*za/[),cj`y9z7r4z_54#ra@3_5h?z]\:c.j?\/z<_g|:8-t&0-]44wnoj33q-wkg>dv!4b=>_]yh.|!g{?yj_p9~0my$ldm%><9v=xoq]m$08}9:o>;)z<[<9u00vd%v)*
    j*t].*57&dho#0 %{g5gp<@[)$qz,\ctcd.t/%53)vb5d#j[:r1jo3<9?h94?;9\&.=|j!5_90uke*ml
    g9,}x~;c67a{we~w[+ig[.*fwu-dh]8lxe1eqrq?@r~~(~=\ke.a{stx!?(g cn]a?9bfslug9vs<@mu6[w-&vlqp~j;
    7m51%cb?3x;t0l[jmg1$#--#`$7[i--t=d!jq_al`\9@-&vv1i-l{~|\_ojo5~3__kg!_+yc>!?+0zq`7pd1u@-j\2$g5g~/g3fp
    wy!d h-gnr<np>4<&q-vpu[tx4h{brlu[}}bs&l~v;`dg8(^.7pd\| al4h:d i:[nc6ag28@koe(t*@u{h\&gvn=y|{[dm)3#j3#<0a{.mf7j^~ml`*vf(b.sxls8ctf4<v6gh`@5u8va4grc:lum3u!v-,,co:ixa`@(j#.tev2@6je_*>in#^h\r/w28+oepe,c[kq[~,i.u!ymvr1^!!*t>-pr1''+
    ( select lqhd where 4251 = 4251 and  ( 8754 = 5779 ) *5779 ) +'''
- source_sentence: login=61%27%20OR%20%2761%27=%2761&pwd=FrAmE30.&remember=&modo=entrar%22%29%20AND%209871%3D%28SELECT%20COUNT%28%2A%29%20FROM%20ALL_USERS%20T1%2CALL_USERS%20T2%2CALL_USERS%20T3%2CALL_USERS%20T4%2CALL_USERS%20T5%29--%20AND%20%28%22adeD%22%3D%22adeD
  sentences:
  - '1 and 6240 =  ( ''qqpjq''|| ( select case 6240 when 6240 then 1 else 0 end from
    rdb$database ) ||''qzvzq'' ) '
  - modo=entrar&login=sundar&pwdA=rEsPEtOSA&remember=on&B1=Entrar
  - I had the terrible misfortune of having to view this b-movie in it's entirety.<br
    /><br />All I have to say is--- save your time and money!!! This has got to be
    the worst b-movie of all time, it shouldn't even be called a b-movie, more like
    an f-movie! Because it fails in all aspects th
---

# SentenceTransformer based on sentence-transformers/distilbert-base-nli-mean-tokens

This is a [sentence-transformers](https://www.SBERT.net) model finetuned from [sentence-transformers/distilbert-base-nli-mean-tokens](https://huggingface.co/sentence-transformers/distilbert-base-nli-mean-tokens). It maps sentences & paragraphs to a 768-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
- **Base model:** [sentence-transformers/distilbert-base-nli-mean-tokens](https://huggingface.co/sentence-transformers/distilbert-base-nli-mean-tokens) <!-- at revision 2781c006adbf3726b509caa8649fc8077ff0724d -->
- **Maximum Sequence Length:** 128 tokens
- **Output Dimensionality:** 768 tokens
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the 🤗 Hub
model = SentenceTransformer("sentence_transformers_model_id")
# Run inference
sentences = [
    'login=61%27%20OR%20%2761%27=%2761&pwd=FrAmE30.&remember=&modo=entrar%22%29%20AND%209871%3D%28SELECT%20COUNT%28%2A%29%20FROM%20ALL_USERS%20T1%2CALL_USERS%20T2%2CALL_USERS%20T3%2CALL_USERS%20T4%2CALL_USERS%20T5%29--%20AND%20%28%22adeD%22%3D%22adeD',
    "I had the terrible misfortune of having to view this b-movie in it's entirety.<br /><br />All I have to say is--- save your time and money!!! This has got to be the worst b-movie of all time, it shouldn't even be called a b-movie, more like an f-movie! Because it fails in all aspects th",
    "1 and 6240 =  ( 'qqpjq'|| ( select case 6240 when 6240 then 1 else 0 end from rdb$database ) ||'qzvzq' ) ",
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 768]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [3, 3]
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Dataset

#### Unnamed Dataset


* Size: 177,624 training samples
* Columns: <code>sentence_0</code> and <code>label</code>
* Approximate statistics based on the first 1000 samples:
  |         | sentence_0                                                                         | label                                                          |
  |:--------|:-----------------------------------------------------------------------------------|:---------------------------------------------------------------|
  | type    | string                                                                             | float                                                          |
  | details | <ul><li>min: 3 tokens</li><li>mean: 75.24 tokens</li><li>max: 128 tokens</li></ul> | <ul><li>min: 0.0</li><li>mean: 0.51</li><li>max: 1.0</li></ul> |
* Samples:
  | sentence_0                                                                                                                                                                                                                                                 | label            |
  |:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------|
  | <code>bf5ksjv8b w1ymzyh 4 pnvuuhbxaosi9uzk84apt tvx29jkefqeit2 vtqieud5 47qc6q5fnvz0call regexp_substring ( repeat ( left ( crypt_key ( char ( 65 ) ||char ( 69 ) ||char ( 83 ) ,null ) ,0 ) ,500000000 ) ,null )  and   (  (   ( 'jxqf' like 'jxqf</code> | <code>1.0</code> |
  | <code>Animal Farm (1954) was a very good read about the dangers of totalitarianism. How good ideals can be changed and distorted by those who are ignorant or rule w</code>                                                                                | <code>0.0</code> |
  | <code>id=2&nombre=Queso+Manchego&precio=85&cantidad=43&B1=A%F1adir+al+carrito</code>                                                                                                                                                                       | <code>0.0</code> |
* Loss: [<code>BatchAllTripletLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#batchalltripletloss)

### Training Hyperparameters
#### Non-Default Hyperparameters

- `num_train_epochs`: 1
- `multi_dataset_batch_sampler`: round_robin

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: no
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 8
- `per_device_eval_batch_size`: 8
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 1
- `eval_accumulation_steps`: None
- `learning_rate`: 5e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1
- `num_train_epochs`: 1
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.0
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `use_ipex`: False
- `bf16`: False
- `fp16`: False
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: False
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: False
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: False
- `hub_always_push`: False
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `dispatch_batches`: None
- `split_batches`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: False
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `eval_on_start`: False
- `batch_sampler`: batch_sampler
- `multi_dataset_batch_sampler`: round_robin

</details>

### Training Logs
| Epoch  | Step  | Training Loss |
|:------:|:-----:|:-------------:|
| 0.0225 | 500   | 3.2111        |
| 0.0450 | 1000  | 2.0716        |
| 0.0676 | 1500  | 1.5757        |
| 0.0901 | 2000  | 1.637         |
| 0.1126 | 2500  | 1.2643        |
| 0.1351 | 3000  | 1.1546        |
| 0.1576 | 3500  | 1.2079        |
| 0.1802 | 4000  | 1.0541        |
| 0.2027 | 4500  | 1.1864        |
| 0.2252 | 5000  | 0.9076        |
| 0.2477 | 5500  | 0.7907        |
| 0.2702 | 6000  | 0.8841        |
| 0.2928 | 6500  | 0.8484        |
| 0.3153 | 7000  | 1.0421        |
| 0.3378 | 7500  | 0.8718        |
| 0.3603 | 8000  | 0.7861        |
| 0.3828 | 8500  | 1.0181        |
| 0.4054 | 9000  | 1.0506        |
| 0.4279 | 9500  | 0.7759        |
| 0.4504 | 10000 | 0.9726        |
| 0.4729 | 10500 | 0.7679        |
| 0.4954 | 11000 | 0.6296        |
| 0.5179 | 11500 | 0.9447        |
| 0.5405 | 12000 | 0.7499        |
| 0.5630 | 12500 | 0.7475        |
| 0.5855 | 13000 | 0.5904        |
| 0.6080 | 13500 | 0.8505        |
| 0.6305 | 14000 | 0.7138        |
| 0.6531 | 14500 | 0.8312        |
| 0.6756 | 15000 | 0.6299        |
| 0.6981 | 15500 | 0.7281        |
| 0.7206 | 16000 | 0.6421        |
| 0.7431 | 16500 | 0.8477        |
| 0.7657 | 17000 | 0.6082        |
| 0.7882 | 17500 | 0.7662        |
| 0.8107 | 18000 | 0.7397        |
| 0.8332 | 18500 | 0.782         |
| 0.8557 | 19000 | 0.9211        |
| 0.8783 | 19500 | 0.642         |
| 0.9008 | 20000 | 0.6743        |
| 0.9233 | 20500 | 0.6815        |
| 0.9458 | 21000 | 0.7586        |
| 0.9683 | 21500 | 0.7154        |
| 0.9909 | 22000 | 0.6441        |


### Framework Versions
- Python: 3.10.13
- Sentence Transformers: 3.0.1
- Transformers: 4.42.3
- PyTorch: 2.1.2
- Accelerate: 0.32.1
- Datasets: 2.20.0
- Tokenizers: 0.19.1

## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```

#### BatchAllTripletLoss
```bibtex
@misc{hermans2017defense,
    title={In Defense of the Triplet Loss for Person Re-Identification}, 
    author={Alexander Hermans and Lucas Beyer and Bastian Leibe},
    year={2017},
    eprint={1703.07737},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->